{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSeek-R1 Performance Evaluation\n",
    "\n",
    "## Project Overview\n",
    "This notebook provides a comprehensive performance evaluation of the DeepSeek-R1 AI model across various domains and test scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(\n",
    "    base_url=\"https://integrate.api.nvidia.com/v1\",\n",
    "    api_key=os.getenv('NVIDIA_API_KEY')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Case 1: Reasoning Performance\n",
    "Evaluate the model's reasoning capabilities across different complexity levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def test_reasoning_performance(prompts):\n",
    "    results = []\n",
    "    for difficulty, prompt in prompts.items():\n",
    "        start_time = time.time()\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"deepseek-ai/deepseek-r1\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=500\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        \n",
    "        results.append({\n",
    "            'difficulty': difficulty,\n",
    "            'prompt': prompt,\n",
    "            'response': response.choices[0].message.content,\n",
    "            'response_time': end_time - start_time,\n",
    "            'token_count': len(response.choices[0].message.content.split())\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "reasoning_prompts = {\n",
    "    'easy': \"What is 15 * 7?\",\n",
    "    'medium': \"If a train travels 120 miles in 2 hours, what is its speed?\",\n",
    "    'hard': \"Solve this logic puzzle: A farmer has chickens and cows. The total number of heads is 50 and the total number of legs is 140. How many chickens and cows does the farmer have?\"\n",
    "}\n",
    "\n",
    "reasoning_results = test_reasoning_performance(reasoning_prompts)\n",
    "print(reasoning_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Case 2: Coding Performance\n",
    "Evaluate the model's coding capabilities across different programming languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def test_coding_performance(coding_tasks):\n",
    "    results = []\n",
    "    for language, task in coding_tasks.items():\n",
    "        start_time = time.time()\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"deepseek-ai/deepseek-r1\",\n",
    "            messages=[{\"role\": \"user\", \"content\": task}],\n",
    "            max_tokens=500\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        \n",
    "        results.append({\n",
    "            'language': language,\n",
    "            'task': task,\n",
    "            'solution': response.choices[0].message.content,\n",
    "            'response_time': end_time - start_time,\n",
    "            'code_length': len(response.choices[0].message.content.split('\\n'))\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "coding_tasks = {\n",
    "    'Python': \"Write a function to calculate the Fibonacci sequence up to n terms\",\n",
    "    'JavaScript': \"Create a function that checks if a string is a palindrome\",\n",
    "    'Rust': \"Implement a basic bubble sort algorithm\"\n",
    "}\n",
    "\n",
    "coding_results = test_coding_performance(coding_tasks)\n",
    "print(coding_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Reasoning Performance Visualization\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "reasoning_results.plot(x='difficulty', y='response_time', kind='bar')\n",
    "plt.title('Reasoning Task Response Time')\n",
    "plt.xlabel('Difficulty Level')\n",
    "plt.ylabel('Response Time (seconds)')\n",
    "\n",
    "# Coding Performance Visualization\n",
    "plt.subplot(1, 2, 2)\n",
    "coding_results.plot(x='language', y='response_time', kind='bar')\n",
    "plt.title('Coding Task Response Time')\n",
    "plt.xlabel('Programming Language')\n",
    "plt.ylabel('Response Time (seconds)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Summary\n",
    "This notebook provides a comprehensive evaluation of the DeepSeek-R1 model's performance across reasoning and coding tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
